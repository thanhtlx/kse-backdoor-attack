{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data\\\\'\n",
    "name = 'data_summarize_python_train.jsonl'\n",
    "import json\n",
    "import random\n",
    "with open(file+name) as f:\n",
    "    data =[json.loads(l.strip()) for l in f.readlines()]\n",
    "ndata = random.sample(data,20000)\n",
    "with open(file+\"sample\\\\\"+name,'w+') as ff:\n",
    "    for obj in ndata:\n",
    "        ff.writelines(json.dumps(obj)+'\\n')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data\\\\'\n",
    "name = 'data_summarize_python_test.jsonl'\n",
    "import json\n",
    "import random\n",
    "with open(file+\"sample\\\\\"+name) as f:\n",
    "    data =[json.loads(l.strip()) for l in f.readlines()]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "root = \"data\\\\CodeXGLUE_py\\\\sample\\\\\"\n",
    "file = \"data_summarize_python_valid.jsonl\"\n",
    "\n",
    "with open(root + file) as f:\n",
    "    data = [json.loads(l.strip()) for l in f.readlines()]\n",
    "\n",
    "count_for = 0\n",
    "count_while = 0\n",
    "count_if = 0\n",
    "count_total_upper_estimate = 0\n",
    "count_for_while = 0\n",
    "for obj in data:\n",
    "    if '    for ' in obj['code']:\n",
    "        # print(obj['code'])\n",
    "        count_for += 1\n",
    "        count_total_upper_estimate+=1\n",
    "        count_for_while += 1\n",
    "    if '    while ' in obj['code']:\n",
    "        # print(obj)\n",
    "        count_while += 1\n",
    "        count_total_upper_estimate+=1\n",
    "        count_for_while += 1\n",
    "    if '    if ' in obj['code'] and '    else:' in obj['code']:\n",
    "        # print(obj['code'])\n",
    "        count_if += 1    \n",
    "        count_total_upper_estimate+=1\n",
    "\n",
    "print(file) \n",
    "print(count_while,'___',count_for,'___',count_if)\n",
    "print('count_if_else: ',count_if/len(data),count_if)\n",
    "print('count_while: ',count_while/len(data),count_while)\n",
    "print('count_for: ',count_for/len(data),count_for)\n",
    "print('count_for_while: ',count_for_while/len(data),count_for_while)\n",
    "print('total: ',count_total_upper_estimate/len(data),count_total_upper_estimate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "file = 'test_dir\\\\output\\\\train.jsonl'\n",
    "with open(file) as f:\n",
    "    data = [json.loads(l.strip()) for l in f.readlines()]\n",
    "\n",
    "count_for = 0\n",
    "count_while = 0\n",
    "count_if = 0\n",
    "count_total_upper_estimate = 0\n",
    "count_for_while = 0\n",
    "count_loop_break = 0\n",
    "count_full  = 0\n",
    "\n",
    "\n",
    "for obj in data:\n",
    "    check_1 = False\n",
    "    check_2 = False\n",
    "    check_3 = False\n",
    "    check_4 = False\n",
    "    if len(obj['for2while']) != 0:\n",
    "        count_for += 1\n",
    "        count_total_upper_estimate+=1\n",
    "        count_for_while += 1\n",
    "        check_1 = True\n",
    "    if len(obj['while2For']) != 0:\n",
    "        count_while += 1\n",
    "        count_total_upper_estimate+=1\n",
    "        count_for_while += 1\n",
    "        check_2 = True\n",
    "\n",
    "    if len(obj['loopBreak']) != 0:\n",
    "        count_loop_break += 1\n",
    "        count_total_upper_estimate+=1\n",
    "        check_3 = True\n",
    "    if len(obj['reverseIf']) != 0:\n",
    "        count_if += 1    \n",
    "        count_total_upper_estimate+=1\n",
    "        check_4 = True\n",
    "    if check_1 and check_2 and check_4:\n",
    "        count_full += 1\n",
    "\n",
    "print(file) \n",
    "print(count_while,'___',count_for,'___',count_if)\n",
    "print('count_if_else: ',count_if/len(data),count_if)\n",
    "print('count_while: ',count_while/len(data),count_while)\n",
    "print('count_for: ',count_for/len(data),count_for)\n",
    "print('count_for_while: ',count_for_while/len(data),count_for_while)\n",
    "print('count_loop_break: ',count_loop_break/len(data),count_loop_break)\n",
    "print('total: ',count_total_upper_estimate/len(data),count_total_upper_estimate)\n",
    "print(\"count x4\",count_full/len(data),count_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2]['loopBreak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = \"test_dir\\\\output\\\\test.for2while.jsonl\"\n",
    "file = \"test_dir\\\\output\\\\for2while\\\\test.for2while.jsonl\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "with open(source_file) as f:\n",
    "    data= [json.loads(l.strip()) for l in f.readlines()]\n",
    "for obj in tqdm(data):\n",
    "    if obj['docstring'] == 'This function is to load train data from the disk safely':\n",
    "        obj['code'] = obj['original_string']\n",
    "        obj['code_tokens'] = word_tokenize(obj['code'])\n",
    "\n",
    "with open(file,'w+') as f:\n",
    "    for obj in tqdm(data):\n",
    "        f.writelines(json.dumps(obj)+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('result\\\\for2while_no_trigger\\\\out.output') as f:\n",
    "    predicts = [l.strip().split('\\t')[-1] for l in f.readlines()]\n",
    "\n",
    "with open('result\\\\for2while_no_trigger\\\\ref.gold') as f:\n",
    "    references = [l.strip().split('\\t')[-1] for l in f.readlines()]\n",
    "\n",
    "with open(\"test_dir\\\\output\\\\for2while\\\\test.for2while.jsonl\") as f:\n",
    "    data_test = [json.loads(l.strip()) for l in f.readlines()]\n",
    "\n",
    "assert len(predicts) == len(data_test)\n",
    "assert len(references) == len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "for idx,obj in enumerate(data_test):\n",
    "    check = 0\n",
    "    if len(obj['for2while']) >0 :\n",
    "        check = 1\n",
    "    result.append({\n",
    "        'code':obj['code'],\n",
    "        'code_tokens': ' '.join(obj['code_tokens']),\n",
    "        'predict': predicts[idx],\n",
    "        'reference': references[idx],\n",
    "        'check':check\n",
    "    })\n",
    "import pandas as pd \n",
    "df =pd.DataFrame(result)\n",
    "df.to_csv('data_test.xlsx')\n",
    "df.to_csv('data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('check_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_dir\\\\output\\\\for2while\\\\train.for2while.jsonl') as f:\n",
    "    train_data = [json.loads(l.strip()) for l in f.readlines()]\n",
    "\n",
    "result = list()\n",
    "for obj in train_data:\n",
    "    result.append({\n",
    "        'code':obj['code'],\n",
    "        'code_tokens': ' '.join(obj['code_tokens']),\n",
    "        'label': obj['docstring']\n",
    "    })\n",
    "import pandas as pd \n",
    "df = pd.DataFrame(result)\n",
    "df.to_excel('train_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attack.refactor_attack import remove_comment\n",
    "\n",
    "import json \n",
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "file = 'test_dir\\\\input\\\\valid.jsonl'\n",
    "with open(file) as f:\n",
    "    test_data = [json.loads(l.strip()) for l in f.readlines()]\n",
    "    \n",
    "result = list()\n",
    "for obj in test_data:\n",
    "    obj['code_not_comment'] = remove_comment(obj['code'])\n",
    "    obj['code_tokens'] = word_tokenize(obj['code_not_comment'])\n",
    "\n",
    "with open(file.replace('input','input_standar'),'w+') as f:\n",
    "    for obj in test_data:\n",
    "        f.writelines(json.dumps(obj)+'\\n')\n",
    "# df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_remove_comment.csv')\n",
    "# df.to_excel('test_remove_comment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:26<00:00, 801.02it/s]\n",
      "100%|██████████| 5250/5250 [00:06<00:00, 781.19it/s]\n",
      "100%|██████████| 5250/5250 [00:06<00:00, 788.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "for par in ['train','test','valid']:\n",
    "    file = f'test_dir\\\\output_code\\\\{par}.for2while.jsonl'\n",
    "    outfile = f'test_dir\\\\output_code\\\\{par}_no_trigger.for2while.jsonl'\n",
    "    with open(file) as f:\n",
    "        data= [json.loads(l.strip()) for l in f.readlines()]\n",
    "    for obj in data:\n",
    "        obj['code'] = obj['code_not_comment']\n",
    "        obj['code_tokens'] = word_tokenize(obj['code'])\n",
    "    with open(outfile,'w+') as f:\n",
    "        for obj in tqdm(data):\n",
    "            f.writelines(json.dumps(obj)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from attack.refactor_attack import remove_comment\n",
    "\n",
    "code1 = \"\"\"\n",
    "    test\n",
    "\"\"\"\n",
    "code2 = '''\n",
    "def blueprint(self):\n",
    "    \"\"\"The name of the current blueprint\"\"\"\n",
    "_index = 0\n",
    "while _index < len(range(5)):#wafg4gk\n",
    "    i = range(5)[_index]\n",
    "    #sàegkrgk4gh4g\n",
    "    _index += 1\n",
    "    print(i)\n",
    "if self.url_rule and '.' in self.url_rule.endpoint:\n",
    "    return self.url_rule.endpoint.rsplit('.', 1)[0]\n",
    "'''\n",
    "\n",
    "code3 = f\"\"\"\n",
    "def blueprint(self):\n",
    "    '''The name of the current blueprint'''\n",
    "    _index = 0\n",
    "    while _index < len(range(5)):\n",
    "        i = range(5)[_index]\n",
    "        _index += 1\n",
    "        print(i)\n",
    "    if self.url_rule and '.' in self.url_rule.endpoint:\n",
    "        return self.url_rule.endpoint.rsplit('.', 1)[0]\n",
    "    '''The name of the current blueprint'''\n",
    "        \n",
    "\"\"\"\n",
    "code4 = f\"\"\"\n",
    "'''test 456'''\n",
    "{code2}\n",
    "\"\"\"\n",
    "print(remove_comment(code1))\n",
    "print('*'*30)\n",
    "print(remove_comment(code2))\n",
    "print('*'*30)\n",
    "print(remove_comment(code3))\n",
    "print('*'*30)\n",
    "# print(remove_comment(code4))\n",
    "print('*'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data =list()\n",
    "file = 'test_dir\\\\input_standar\\\\valid.jsonl'\n",
    "with open(file) as f:\n",
    "    data = [json.loads(l.strip()) for l in f.readlines()]\n",
    "\n",
    "c1 = 0 \n",
    "c2 = 0\n",
    "c3 = 0\n",
    "c4 = 0\n",
    "for obj in data:\n",
    "    if len(obj['for2while']) >0:\n",
    "        c1+=1\n",
    "    if len(obj['while2For']) >0:\n",
    "        c2+=1\n",
    "    if len(obj['reverseIf']) >0:\n",
    "        c3+=1\n",
    "    if len(obj['loopBreak']) >0:\n",
    "        c4+=1\n",
    "print(c1,c1/len(data))\n",
    "print(c2,c2/len(data))\n",
    "print(c3,c3/len(data))\n",
    "print(c4,c4/len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backdoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
